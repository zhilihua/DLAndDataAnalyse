用的数据是比赛数据，数据中存在一定的噪声，所以没有用标准库数据的精度高，下面详解下如何一步步将精度提升。
一、搭建简单的CNN版本，不做任何的优化调参。
    对于机器学习问题，一般操作是搭建一个简单的模型，这样就可以简单的了解下任务的难度，然后基于这个baseline
进行修改调整。首先设计模型如下：
    模型的输入是一个4维的tensor，尺寸大小（64， 224， 224， 3），分别表示一批图片的个数128、图片的宽的像素
点个数32、高的像素点个数32和信道个数3。首先使用多个卷积神经网络层进行图像的特征提取，卷积神经网络层的计算过程如下步骤：
    卷积层1：卷积核大小3*3，卷积核移动步长1，卷积核个数64，池化大小2*2，池化步长2，池化类型为最大池化，激活函数ReLU。
    卷积层2：卷积核大小3*3，卷积核移动步长1，卷积核个数64，池化大小2*2，池化步长2，池化类型为最大池化，激活函数ReLU。
    卷积层3：卷积核大小3*3，卷积核移动步长1，卷积核个数128，池化大小2*2，池化步长2，池化类型为最大池化，激活函数ReLU。
    卷积层4：卷积核大小3*3，卷积核移动步长1，卷积核个数128，池化大小2*2，池化步长2，池化类型为最大池化，激活函数ReLU。
    卷积层5：卷积核大小3*3，卷积核移动步长1，卷积核个数256，池化大小2*2，池化步长2，池化类型为最大池化，激活函数ReLU。
    全连接层1：隐藏层单元数256，激活函数ReLU。
    全连接层2：隐藏层单元数128，激活函数ReLU。
    分类层：隐藏层单元数5，激活函数softmax。
    注：这里设计5层主要是根据最终生成的特征层尺寸来设计的，普通网络一般为conv+pool结构，为了避免最终的参数过多，我们
令最终特征层为7x7.所以7*2^5=224，或者可以将卷积层的步长调大，达到特征尺寸降低的目的。
    参数初始化采用he_normal模式，使用cross entropy作为目标函数，使用Adam梯度下降法进行参数更新，学习率设为固定值0.0003。
    该网络是一个有五层卷积层的神经网络，能够快速地完成图像地特征提取。全连接层用于将图像特征整合成分类特征，分类层用于分类。
cross entropy也是最常用的目标函数之一，分类任务使用cross entropy作为目标函数非常适合。Adam梯度下降法也是现在非常流行的
梯度下降法的改进方法之一，学习率过大会导致模型难以找到较优解，设置过小则会降低模型训练效率，因此选择适中的0.0003（可以根据自己
实际情况进行调整）。这样，我们最基础版本的CNN模型就已经搭建好了，接下来进行训练和测试以观察结果。
    经过200轮，模型的损失值、训练集准确率、验证集准确率变化如下。训练集准确率为：xxxx,验证集准确率为：xxxx。(存在过拟合，
可以根据自己数据进行设置训练集、验证集、测试集，由于数据量不大，所以不设置测试集)
    补充图片。。。。。。。。。。。
    结果分析：首先我们观察训练loss（目标函数值）变化，刚开始loss从200不断减小到接近0，但是在100轮左右开始出现震荡，并且随着训练
幅度越来越大，说明模型不稳定。然后观察训练集和验证集的准确率，发现训练集准确率接近于1，验证集准确率稳定在70%左右，说明模型的泛化
能力不强并且出现了过拟合情况。最后评估测试集，发现准确率为69.36%，也没有达到很满意的程度，说明我们对模型需要进行很大的改进，
接下来进行漫长的调参之旅吧！（需要修改，作为参考）
二、进行数据增强，降低过拟合。
    使用数据增强技术（封装keras自带的ImageDataGenerator），主要是在训练数据上增加微小的扰动或者变化，一方面可以增加训练数据，
从而提升模型的泛化能力，另一方面可以增加噪声数据，从而增强模型的鲁棒性。主要的数据增强方法有：翻转变换 flip、随机修剪（random crop）、
色彩抖动（color jittering）、平移变换（shift）、尺度变换（scale）、对比度变换（contrast）、噪声扰动（noise）、旋转变换/反射变换
（rotation/reflection）等，可以参考ImageDataGenerator参数说明。获取一个batch的训练数据，进行数据增强步骤之后再送入网络进行训练。
    我主要做的数据增强操作有如下方面：
    1、图像角度旋转：图像按照逆时针方向旋转一定角度。
    2、图像宽度平移：将图像的宽度平移一定距离。
    3、图像高度平移：同上。
    4、图像剪切强度处理：图像按照逆时针旋转一定角度，但这个角度为仿射角度。
    5、图像整体缩放：略。
    6、上下、左右翻转：略。
    7、数据预处理：按照某一规则，将数据进行处理。
    经过200轮，模型的损失值、训练集准确率、验证集准确率变化如下。训练集准确率为：xxxx,验证集准确率为：xxxx。
    补充图片。。。。。。。。。。。
    结果分析：我们观察训练曲线和验证曲线，很明显地发现图像白化的效果好，其次是图像切割，再次是图像翻转，而如果同时使用这三种数据
增强技术，不仅能使训练过程的loss更稳定，而且能使验证集的准确率提升至82%左右，提升效果十分明显。而对于测试集，准确率也提升至80.42%。
说明图像增强确实通过增加训练集数据量达到了提升模型泛化能力以及鲁棒性的效果，从准确率上看也带来了将近10%左右的提升，因此，
数据增强确实有很大的作用。但是对于80%左右的识别准确率我们还是不够满意，接下来继续调参。（需要修改，作为参考）
三、从模型入手，使用一些改进方法
    接下来的步骤是从模型角度进行一些改进，由于某一个特定问题对某一个模型的改进千变万化，没有办法全部去尝试，因此一般会尝试
一些通用的方法，比如批正则化（batch normalization）、权重衰减（weight decay）、dropout。我这里实验了3种改进方法，接下来依次介绍。
    权重衰减（weight decay）：对于目标函数加入正则化项，限制权重参数的个数，这是一种防止过拟合的方法，这个方法其实就是机器学习中
的l2正则化方法。
    dropout：在每次训练的时候，让某些的特征检测器停过工作，即让神经元以一定的概率不被激活，这样可以防止过拟合，提高泛化能力。
    批正则化（batch normalization）：batch normalization对神经网络的每一层的输入数据都进行正则化处理，这样有利于让数据的
分布更加均匀，不会出现所有数据都会导致神经元的激活，或者所有数据都不会导致神经元的激活，这是一种数据标准化方法，能够提升模型的拟合能力。
    经过200轮，模型的损失值、训练集准确率、验证集准确率变化如下。训练集准确率为：xxxx,验证集准确率为：xxxx。
    补充图片。。。。。。。。。。。
    结果分析：我们观察训练曲线和验证曲线，随着每一个模型提升的方法，都会使训练集误差和验证集准确率有所提升，其中，批正则化技术和
dropout技术带来的提升非常明显，而如果同时使用这些模型提升技术，会使验证集的准确率从82%左右提升至88%左右，提升效果十分明显。
而对于测试集，准确率也提升至85.72%。我们再注意看左图，使用batch normalization之后，loss曲线不再像之前会出现先下降后上升的情况，
而是一直下降，这说明batch normalization技术可以加强模型训练的稳定性，并且能够很大程度地提升模型泛化能力。所以，
如果能提出一种模型改进技术并且从原理上解释同时也使其适用于各种模型，那么就是非常好的创新点，也是我想挑战的方向。
现在测试集准确率提升至85%左右。（需要修改，作为参考）
四、其他的调整方法。
    1、变化的学习率，进一步提升模型性能
    2、加深网络层数
    由于我们这里用的是十分优秀的Adam算法，其自身会对学习率进行衰减，因此没有进行尝试。由于我们的数据量太少，所以没有对模型进一步的
加深，你可以根据自己的数据进行适当的增减。
五、总结
    我们从最简单的卷积神经网络开始，分类准确率只能达到xxx%左右，通过不断地增加提升模型性能的方法，最终将分类准确里提升到了91%左右
（最好的模型权重），这20%多的准确率的提升来自于对数据的改进、对模型的改进、对训练过程的改进等，具体每一项提升如下表所示。
    改进方法 获得准确率 提升
    基本神经网络 xxxx% -
    +数据增强 xxxx% xxxx%
    +模型改进 xxxx% xxxx%
    +变化学习率 xxxx% xxxx%
    +深度残差网络 xxxx% xxxx%